#!/usr/bin/env python3
import datetime
import hashlib
import itertools
import os
import shutil
import sys
import logging

import exifread
from hachoir.parser import createParser
from hachoir.metadata import extractMetadata

logging.getLogger("hachoir").setLevel(logging.ERROR)
logging.getLogger("exifread").setLevel(logging.ERROR)
logfile = "%s.log" % datetime.datetime.now()
logging.basicConfig(format='%(levelname)s : %(message)s', filename=logfile, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler())

exclude_ext = ['tec', 'dup']
mim_file_size = 1000


def exif(media):
    with open(media, 'rb') as f:
        metadata = exifread.process_file(f, details=False)
        dt = metadata['Image DateTime']
        dt = datetime.datetime.strptime(str(dt), '%Y:%m:%d %H:%M:%S')
        return dt


def hachoir(media):
    parser = createParser(media)
    metadata = extractMetadata(parser)
    if metadata:
        dt = metadata.exportDictionary()['Metadata']['Creation date']
        dt = datetime.datetime.strptime(str(dt), '%Y-%m-%d %H:%M:%S')
        return dt


def strpfilename(media):
    dt = os.path.basename(media).split('.')[0]
    for f in ['%Y%m%d_%H%M%S', '%Y-%m-%d %H:%M:%S']:
        try:
            return datetime.datetime.strptime(str(dt), f)
        except Exception:
            continue
    logging.warning("no date extracted for media %s", media)


def parse(func, media):
    try:
        dt = func(media)
        if dt.year < 1980:
            raise Exception("date before 1980 : %s" % dt)
        else:
            return dt.year, dt.month, dt.isocalendar()[1]
    except Exception as e:
        logging.error("%s : %s : %s", media, func.__name__, str(e))
        return None


def walk(idir):
    trash = os.path.join(idir, 'trash')
    os.makedirs(trash, exist_ok=True)
    for root, dirs, files in os.walk(idir):
        for f in files:
            if f.split('.')[-1] in exclude_ext:
                continue
            filepath = os.path.join(root, f)
            if os.stat(filepath).st_size < mim_file_size:
                os.rename(f, os.path.join(trash, "%s_%s" % (f, md5(filepath))))
                continue
            yield filepath


def extract_date(idir):
    for media in walk(idir):
        dt = parse(exif, media)
        if not dt:
            dt = parse(hachoir, media)
        if not dt:
            dt = parse(strpfilename, media)
        logging.debug("%s %s", media, dt)
        if dt:
            yield media, dt


def md5(media):
    hasher = hashlib.md5()
    with open(media, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()


def get_output_filepath(of):
    for i in itertools.count():
        if not os.path.exists(of):
            return of
        else:
            split = of.split('.')
            split[-2] += '_%d' % i
            of = '.'.join(split)


def main():
    idir = sys.argv[1]
    for f, (year, month, weeknb) in extract_date(idir):
        year, month, weeknb = map(str, (year, month, weeknb))
        dst = os.path.join(idir, year, month, weeknb)
        os.makedirs(dst, exist_ok=True)
        if md5(f) in [md5(os.path.join(dst, o)) for o in os.listdir(dst)]:
            shutil.move(f, f + ".dup")
            logging.info("move %s to %s", f, f + ".dup")
            logging.warning("duplicate file %s in %s", f, dst)
        else:
            of = get_output_filepath(os.path.join(dst, os.path.basename(f)))
            shutil.move(f, of)
            logging.info("move %s to %s", f, of)


if __name__ == '__main__':
    main()
